---
title: "Predict movie rating"
author: "Mario R. Melchiori"
date: "1/5/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE, cache= TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Recommendation systems use rating data from many products and users to make recommendations for a specific user. 

Netflix uses a recommendation system to predict your ratings for a specific movie.

On October 2006 Netflix offered a challenge to the data science community: improve our recommendation algorithm by 10% and win a million dollars. In September 2009 
[the winners were announced](http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/).

In this capstone, we will build our own recommendation system.

Below, we will try out different models.

We keep track of the RMSE we get for each one because we will report the best (lower) on validation set, at the end.

The definition of root mean square error (RMSE) is square root of the average of the residuals squared:

$$\mbox{RMSE} = \sqrt{\frac{1}{N}\sum_{i=1}^N (\hat{Y}_i - Y_i)^2}$$

where $Y_i$ is the true $i$-th movie rating and $\hat{Y}_i$ our predicted rating. 
We can interpret this similarly to a standard deviation.
It is the typical error we make when predicting a movie rating.

We write a function called `RMSE()` that takes two numeric vectors
(one corresponding to the true movie ratings, and one 
corresponding to predicted movie ratings) as input, and returns 
the root mean square error (RMSE) between the two as output. 

The definition of root mean square error is square root of the 
average of the residuals squared:

```{r}
## RMSE compute root mean square error (RMSE)
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

RMSE was the metric used to judge entries in the Netflix challenge.
The lower the RMSE was on Netflix's quiz set between the submitted 
rating predictions and the actual ratings, the better the method was. 
We will be using RMSE to evaluate our machine learning models in 
this capstone as well.

## The data

We download the MovieLens data and run the following code the staff provided to generate the datasets. We use the 10M version of the MovieLens dataset available [here] (https://grouplens.org/datasets/movielens/10m/).


```{r}
#############################################################
# Create edx set, validation set, and submission file
#############################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
     semi_join(edx, by = "movieId") %>%
     semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```



*The data set is very long, so we use models that allow the data set can be adjusted and processed within the available RAM in a machine*.

We randomly split the `edx` set into training and test data.
We use `set.seed(1)` to provide a reproducible example and the `createDataPartition`'s caret function to select your test index to create two separate data frames called: `train` and `test` from the original `edx` data frame. `test` contain a randomly selected 10% of the rows of `edx`, and have `train` contain the other 90%. We will use these data frames to do the rest of the analyses. 
In the code, we make sure userId and movieId in test set are also in train set and add rows removed from `test` set back into `train` set.
After you create `train` and `test`, we remove `edx`,`removed`,`temp`,`test_index` to save space. 

```{r}
# generate reproducible partition

set.seed(1)

# test set will be 10% of edx data

test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train <- edx[-test_index,]
temp <- edx[test_index,]

# Make sure userId and movieId in test set are also in train set

test <- temp %>% 
  semi_join(train, by = "movieId") %>%
  semi_join(train, by = "userId")

# Add rows removed from test set back into train set

removed <- anti_join(temp, test)
train <- rbind(train, removed)
```

The code below is to save space.

```{r}
# to save space
rm(removed, temp, test_index) 
```

## Data Exploration
We can see this table is in tidy format with thousands of rows:

```{r}
train %>% as_tibble()
```

Each row represents a rating given by one user to one movie. 
We can see the number of unique users from `train` that provided ratings and how many unique movies were rated. How many users are there in the `train` data set? How many movies are in the `train` data set? What is the lowest rating in the `train` data set? The highest? 

```{r}
train %>% summarize(
  n_users=n_distinct(userId),
  n_movies=n_distinct(movieId),
  min_rating=min(rating),  
  max_rating=max(rating))
```

If we multiply `n_users` times `n_movie`s, we get a number almost of 750 million, yet our data table has about 8,000,000 rows. This implies that not every user rated every movie. So we can think of these data as a very large matrix, with users on the rows and movies on the columns, with many empty cells. The `gather` function permits us to convert it to this format, but if we try it for the entire matrix, it will crash R. Let's show the matrix for seven users and five movies.

```{r}
keep <- train %>% 
  count(movieId) %>% 
  top_n(5, n) %>% 
  .$movieId

tab <- train %>% 
  filter(movieId%in%keep) %>% 
  filter(userId %in% c(13:20)) %>% 
  select(userId, title, rating) %>% 
  mutate(title = str_remove(title, ", The"),
         title = str_remove(title, ":.*")) %>%
  spread(title, rating)
tab %>% knitr::kable()
```

You can think of the task of a recommendation system as filling in the `NA`s in the table above. To see how _sparse_ the matrix is, here is the matrix for a random sample of 100 movies and 100 users with yellow indicating a user/movie combination for which we have a rating.

```{r}
users <- sample(unique(train$userId), 100)
rafalib::mypar()
train %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users")
abline(h=0:100+0.5, v=0:100+0.5, col = "grey")
```

The first thing we notice is that some movies get rated more than others. Here is the distribution:

```{r}
train %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "blue") + 
  scale_x_log10() + 
  ggtitle("Movies")
```

Our second observation is that some users are more active than others at rating movies:

```{r}
train %>% 
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "orange") + 
  scale_x_log10() + 
  ggtitle("Users")
```

The code below is to save space.

```{r}
# to save space
rm(tab, keep, users)
```

## Methods and Analysis

### More simplest model

We start by building the simplest possible recommendation system: we predict the same rating for all movies regardless of user. What number should this prediction be? We can use a model based approach to answer this. A model that assumes the same rating for all movies and users with all the differences explained by random variation would look like this:
Let's start by building the simplest possible recommendation system: we predict the same rating for all movies regardless of user. What number should this prediction be? We can use a model based approach to answer this. A model that assumes the same rating for all movies and users with all the differences explained by random variation would look like this:

$$
Y_{u,i} = \mu + \varepsilon_{u,i}
$$
```{r}
mu <- mean(train$rating) # compute mean rating
mu

naive_rmse <- RMSE(test$rating, mu) # compute root mean standard error
naive_rmse
```

From looking at the distribution of ratings, we can visualize that this is the standard deviation of that distribution. We get a RMSE about 1.06. To win the grand prize of $1,000,000, a participating team had to get an RMSE of about 0.857. So we can definitely do much better! 

As we will be comparing different approaches, we create a results table with this naive approach:

```{r}
rmse_results <- tibble(method = "Just the average", RMSE = naive_rmse)
rmse_results %>% knitr::kable()
```

### Modeling movie effects

We know from experience that some movies are just generally rated higher than others. This intuition, that different movies are rated differently, is confirmed by data. We can augment our previous model by adding the term $b_i$ to represent average ranking for movie $i$: 

$$
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
$$

Statistics textbooks refer to to the $b$s as effects. However, in the Netflix challenge papers, they refer to them as "bias", thus the $b$ notation.

We can again use least squares to estimate the $b_i$ in the following way, 

```{r, eval=FALSE}
fit <- lm(rating ~ as.factor(movieId), data = train)
```

but because there are thousands of $b_i$ as each movie gets one, the `lm()` function will be very slow here if not imposible to run.

In this particular situation, we know that the least square estimate $\hat{b}_i$ is just the average of $Y_{u,i} - \hat{\mu}$ for each movie $i$. So we can compute them this way:

```{r}
mu <- mean(train$rating) 
movie_avgs <- train %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))
movie_avgs %>% qplot(b_i, geom ="histogram", bins = 10, data = ., color = I("black"))
```

Let's see how much our prediction improves once we use $\hat{y}_{u,i} = \hat{\mu} + \hat{b}_i$:

```{r}
predicted_ratings <- mu + test %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$b_i

model_1_rmse <- RMSE(predicted_ratings, test$rating)
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Movie Effect Model on test set",  
                                     RMSE = model_1_rmse ))
rmse_results %>% knitr::kable()
```

We already see an improvement. But can we make it better?

### Modeling User effects

Let's compute the average rating for user $u$: 


```{r}
train %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating - mu)) %>% 
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "blue")
```

Notice that there is variability across users as well: some users are very cranky and others love every movie. This implies that a further improvement to our model may be:

$$ 
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
$$

where $b_u$ is a user-specific effect. Now if a cranky user (negative $b_u$) rates a great movie (positive $b_i$), the effects counter each other and we may be able to correctly predict that this user gave this great movie a 3 rather than a 5. 

To fit this model, we could again use `lm` like this:

```{r, eval = FALSE}
lm(rating ~ as.factor(movieId) + as.factor(userId))
```

but, for the reasons described earlier, we won't. Instead, we will compute an approximation by computing $\hat{\mu}$ and $\hat{b}_i$ and estimating $\hat{b}_u$ as the average of $y_{u,i} - \hat{\mu} - \hat{b}_i$:


```{r}
user_avgs <- train %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
```

We can now construct predictors and see how much the RMSE improves:

```{r}
predicted_ratings <- test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

model_2_rmse <- RMSE(predicted_ratings, test$rating)
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Movie and User Effects Model on test set",  
                                     RMSE = model_2_rmse ))
rmse_results %>% knitr::kable()
```

### Regularization

Regularization permits us to penalize large estimates that are formed using small sample sizes.

These are noisy estimates that we should not trust, especially when it comes to prediction. Large errors can increase our RMSE, so we would rather be conservative when unsure.

Let's look at the top 10 worst and best movies based on $\hat{b}_i$. First, let's create a database that connects `movieId` to movie title:

```{r}
# Regularization

# connect movieId to movie title
movie_titles <- train %>% 
  select(movieId, title) %>%
  distinct()
```

Here are the 10 best movies according to our estimate:

```{r}
# top 10 best movies based on b_i
movie_avgs %>% left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i) %>% 
  slice(1:10) %>%  
  knitr::kable()
```

And here are the 10 worst:  

```{r}
# top 10 worse movies based on b_i
movie_avgs %>% left_join(movie_titles, by="movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i) %>% 
  slice(1:10) %>%  
  knitr::kable()
```

When often the best are rated:

```{r}
# add number of rating of the "best" obscure movies
train %>% count(movieId) %>% 
  left_join(movie_avgs) %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

When often the worse are rated:

```{r}
# add number of rating of the "worse" obscure movies

train %>% count(movieId) %>% 
  left_join(movie_avgs) %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

The supposed "best" and "worst" movies were rated by very few users, in most cases just 1. These movies were mostly obscure ones. This is because with just a few users, we have more uncertainty. Therefore, larger estimates of $b_i$, negative or positive, are more likely.

#### Choosing the penalty terms

We use regularization for the estimate both movie and user effects. We are minimizing:

$$
\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u \right)^2 + 
\lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right)
$$

Here we use cross-validation to pick a $\lambda$:

```{r}
lambda <- seq(0, 10, 0.25)

rmses <- sapply(lambda, function(l){
  mu <- mean(train$rating)
  
  b_i <- train %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- train %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- 
    train %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred
  
  return(RMSE(train$rating, predicted_ratings))
})

qplot(lambda, rmses)  

lambda <- lambda[which.min(rmses)]
lambda
```

We use the test set for the final assessment of our third model

```{r}
b_i <- test %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda))

b_u <- test %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))
  
predicted_ratings <- 
    test %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
 
model_3_rmse <- RMSE(test$rating, predicted_ratings)

rmse_results <- bind_rows(rmse_results,
                          tibble(method="Reg Movie and User Effect Model on test set",  
                                 RMSE = model_3_rmse))
rmse_results %>% knitr::kable()
```

### Matrix factorization

We have described how the model:

$$ 
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
$$

accounts for movie to movie differences through the $b_i$ and user to user differences through the $b_u$. But this model leaves out an important source of variation related to the fact that groups of movies have similar rating patterns and groups of users have similar rating patterns as well. We will discover these patterns by studying the residuals:

$$
r_{u,i} = y_{u,i} - \hat{u} - \hat{b}_i - \hat{b}_u
$$

We compute the residuals for train and test set:

```{r}
train <- train %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(res = rating - mu - b_i - b_u)

test <- test %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(res = rating - mu - b_i - b_u)
```

Matrix Factorization is a popular technique to solve recommender system problem. The main idea is to approximate the matrix $R_{m,n}$ by the product of two matrixes of lower dimension: $P_{k, m}$  and  $Q_{k, n}$.

Matrix P represents latent factors of users. So, each k-elements column of matrix P represents each user. Each k-elements column of matrix Q represents each item. So, to find rating for item i by user u we simply need to compute two vectors: $P[,u]’$ x $Q[,i]$. Short and full description of package is available [here](https://rpubs.com/tarashnot/recommender_comparison).

This package works with data saved on disk in 3 columns with no headers.

The usage of recosystem is quite simple, mainly consisting of the following steps:

 - Create a model object (a Reference Class object in R) by calling `Reco()`.
 - (Optionally) call the `$tune()` method to select best tuning paramete.rs along a set of candidate values.
 - Train the model by calling the `$train()` method. A number of parameters can be set inside the function, possibly   coming from the result of `$tune()`.
 - (Optionally) export the model via `$output()`, i.e. write the factorization matrices $P$ and $Q$ into files or return them as R objects.
 - Use the `$predict()` method to compute predicted values.

```{r}

# Using recosystem 
# Install/Load recosystem
if(!require(recosystem)) install.packages("recosystem", repos = "http://cran.us.r-project.org")

train_data <- data_memory(user_index = train$userId, item_index = train$movieId, 
                          rating = train$res, index1 = T)
  
test_data <- data_memory(user_index = test$userId, item_index = test$movieId, index1 = T)
  
recommender <- Reco()


```

We use cross validation to tune the model parameters. Choices for parameters were optimized using RMSE as a loss function. The code does not run here as it takes over one hours to optimize, but we show the oput of the code.

```{r}
set.seed(1) # This is a randomized algorithm

# res = recommender$tune(
#     train_data,
#     opts = list(dim = c(10, 20, 30),
#                 costp_l1 = 0, costq_l1 = 0,
#                 lrate = c(0.05, 0.1, 0.2), nthread = 2)
# )
#  res[["min"]]
# $dim
# [1] 30
# 
# $costp_l1
# [1] 0
# 
# $costp_l2
# [1] 0.01
# 
# $costq_l1
# [1] 0
# 
# $costq_l2
# [1] 0.1
# 
# $lrate
# [1] 0.1
# 
# $loss_fun
# [1] 0.8007686

```

The following code trains a recommender model. It will read from a training data source and create a model file at the specified location. The model file contains necessary information for prediction.

```{r}
# Training model from a data file
set.seed(1) # This is a randomized algorithm
suppressWarnings(recommender$train(train_data, opts = c(dim = 30, costp_l1 = 0,
                                                        costp_l2 = 0.01, costq_l1 = 0,
                                                        costq_l2 = 0.1, lrate = 0.1,
                                                        verbose = FALSE)))
```

We predict unknown entries in the rating matrix on test set.

```{r}
predicted_ratings <- recommender$predict(test_data, out_memory()) + mu + test$b_i + test$b_u 
```

We use the test set for the final assessment

```{r}  
ind <- which(predicted_ratings > 5)
predicted_ratings[ind] <- 5
  
ind <- which(predicted_ratings < 0.5)
predicted_ratings[ind] <- 0.5
  
model_4_rmse <- RMSE(test$rating, predicted_ratings)
  
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Reg. Movie and User + Matrix Fact. on test set",  
                                   RMSE = model_4_rmse))
rmse_results %>% knitr::kable()

```

Lastly, to meet the requirements of the project we calculate the final RMSE on the validation dataset.
We will use the edx dataset provided that ensures that it contains the users who are in validation set.

```{r}

b_i <- edx %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda))

b_u <- edx %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))

edx <- edx %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(res = rating - mu - b_i - b_u)

validation <- validation %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(res = rating - mu - b_i - b_u)

# Using recosystem 
train_data <- data_memory(user_index = edx$userId, item_index = edx$movieId, 
                          rating = edx$res, index1 = T)
  
test_data <- data_memory(user_index = validation$userId, item_index = validation$movieId, index1 = T)
  
recommender <- Reco()
set.seed(1)

suppressWarnings(recommender$train(train_data, opts = c(dim = 30, costp_l1 = 0,
                                                        costp_l2 = 0.01, costq_l1 = 0,
                                                        costq_l2 = 0.1, lrate = 0.1,
                                                        verbose = FALSE)))


predicted_ratings <- recommender$predict(test_data, out_memory()) + mu + 
                       validation$b_i + validation$b_u 

  
ind <- which(predicted_ratings > 5)
predicted_ratings[ind] <- 5
  
ind <- which(predicted_ratings < 0.5)
predicted_ratings[ind] <- 0.5
  
model_5_rmse <- RMSE(validation$rating, predicted_ratings)
  
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Reg. Movie and User + Matrix Fact. on validation set",  
                                   RMSE = model_5_rmse))
rmse_results %>% knitr::kable()

```

